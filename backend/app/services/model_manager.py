"""
æ¨¡å‹è¿›ç¨‹ç®¡ç†å™¨ - æ ¸å¿ƒç»„ä»¶
"""
import asyncio
import subprocess
import time
from datetime import datetime
from typing import Dict, Optional, List
import psutil
import signal
from pathlib import Path

from app.types.schemas import ModelInfo, ModelStatus, DeployRequest
from app.core.config import settings


class ModelInstance:
    """æ¨¡å‹å®ä¾‹"""
    
    def __init__(
        self,
        model_id: str,
        model_name: str,
        port: int,
        parameters: Dict,
    ):
        self.model_id = model_id
        self.model_name = model_name
        self.port = port
        self.parameters = parameters
        self.process: Optional[subprocess.Popen] = None
        self.pid: Optional[int] = None
        self.status = ModelStatus.INITIALIZING
        self.start_time: Optional[datetime] = None
        self.error_message: Optional[str] = None
        self.log_buffer: List[str] = []
        
    def to_model_info(self) -> ModelInfo:
        """è½¬æ¢ä¸º ModelInfo"""
        return ModelInfo(
            id=self.model_id,
            model_name=self.model_name,
            status=self.status,
            pid=self.pid,
            port=self.port,
            start_time=self.start_time,
            error_message=self.error_message,
            parameters=self.parameters,
        )


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    
    _instance = None
    _instances: Dict[str, ModelInstance] = {}
    _used_ports: set = set()
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, "_initialized"):
            self._initialized = True
            self._instances = {}
            self._used_ports = set()
            self._next_port = settings.VLLM_BASE_PORT
    
    def _allocate_port(self, preferred_port: Optional[int] = None) -> int:
        """åˆ†é…å¯ç”¨ç«¯å£"""
        if preferred_port and preferred_port not in self._used_ports:
            self._used_ports.add(preferred_port)
            return preferred_port
        
        # è‡ªåŠ¨åˆ†é…
        while self._next_port in self._used_ports:
            self._next_port += 1
        
        port = self._next_port
        self._used_ports.add(port)
        self._next_port += 1
        return port
    
    def _generate_model_id(self, model_name: str, port: int) -> str:
        """ç”Ÿæˆæ¨¡å‹å®ä¾‹ ID"""
        model_short_name = model_name.split("/")[-1]
        return f"{model_short_name}-{port}"
    
    async def deploy_model(self, request: DeployRequest) -> ModelInfo:
        """
        éƒ¨ç½²æ¨¡å‹
        """
        # æ£€æŸ¥å®ä¾‹æ•°é‡é™åˆ¶
        if len(self._instances) >= settings.VLLM_MAX_INSTANCES:
            raise ValueError(f"å·²è¾¾åˆ°æœ€å¤§å®ä¾‹æ•°é‡é™åˆ¶: {settings.VLLM_MAX_INSTANCES}")
        
        # åˆ†é…ç«¯å£
        port = self._allocate_port(request.port)
        
        # ç”Ÿæˆå®ä¾‹ ID
        model_id = self._generate_model_id(request.model_name, port)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if model_id in self._instances:
            raise ValueError(f"æ¨¡å‹å®ä¾‹ {model_id} å·²å­˜åœ¨")
        
        # åˆ›å»ºå®ä¾‹å¯¹è±¡
        instance = ModelInstance(
            model_id=model_id,
            model_name=request.model_name,
            port=port,
            parameters=request.parameters or {},
        )
        
        self._instances[model_id] = instance
        
        # å¼‚æ­¥å¯åŠ¨è¿›ç¨‹
        asyncio.create_task(self._start_vllm_process(instance, request))
        
        return instance.to_model_info()
    
    async def _start_vllm_process(self, instance: ModelInstance, request: DeployRequest):
        """
        å¯åŠ¨ vLLM è¿›ç¨‹
        """
        try:
            instance.status = ModelStatus.STARTING
            instance.start_time = datetime.now()
            
            # æ„å»º vLLM å‘½ä»¤
            model_path = request.local_path or request.model_name
            
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", model_path,
                "--port", str(instance.port),
                "--host", "0.0.0.0",
            ]
            
            # ğŸ”§ GPU/CPU Mode Configuration
            if settings.USE_GPU:
                print(f"ğŸš€ Deploying {model_path} on GPU")
                # GPU-specific parameters
                params = instance.parameters
                if "dtype" in params:
                    cmd.extend(["--dtype", params["dtype"]])
                if "gpu_memory_utilization" in params:
                    cmd.extend(["--gpu-memory-utilization", str(params["gpu_memory_utilization"])])
            else:
                print(f"ğŸ–¥ï¸  Deploying {model_path} on CPU (testing mode - minimal memory)")
                # CPU-specific parameters with minimal memory usage
                cmd.extend([
                    "--device", "cpu",
                    "--dtype", "float32",
                    "--swap-space", "0",  # Disable swap space
                    "--max-num-seqs", "1",  # Process one request at a time
                    "--enforce-eager",  # Disable CUDA graphs
                ])
                # Note: GPU memory utilization is ignored in CPU mode
            
            # Common parameters
            params = instance.parameters
            # For CPU mode, override max_model_len to a smaller value if not specified
            max_len = params.get("max_model_len", 4096)
            if not settings.USE_GPU and max_len > 2048:
                max_len = 2048  # Limit context length in CPU mode to save memory
                print(f"âš ï¸  Reduced max_model_len to {max_len} for CPU mode")
            cmd.extend(["--max-model-len", str(max_len)])
            
            if params.get("trust_remote_code"):
                cmd.append("--trust-remote-code")
            
            # å¯åŠ¨è¿›ç¨‹
            instance.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True,
            )
            
            instance.pid = instance.process.pid
            instance.status = ModelStatus.RUNNING
            
            # å¼‚æ­¥è¯»å–æ—¥å¿—
            asyncio.create_task(self._read_process_logs(instance))
            
        except Exception as e:
            instance.status = ModelStatus.FAILED
            instance.error_message = str(e)
            print(f"âŒ å¯åŠ¨æ¨¡å‹å¤±è´¥: {e}")
    
    async def _read_process_logs(self, instance: ModelInstance):
        """è¯»å–è¿›ç¨‹æ—¥å¿—"""
        try:
            if not instance.process or not instance.process.stdout:
                return
            
            loop = asyncio.get_event_loop()
            
            while True:
                line = await loop.run_in_executor(
                    None, instance.process.stdout.readline
                )
                
                if not line:
                    # è¿›ç¨‹ç»“æŸ
                    if instance.process.poll() is not None:
                        instance.status = ModelStatus.STOPPED
                        break
                    await asyncio.sleep(0.1)
                    continue
                
                # ä¿å­˜æ—¥å¿—
                instance.log_buffer.append(line.strip())
                
                # é™åˆ¶æ—¥å¿—ç¼“å†²åŒºå¤§å°
                if len(instance.log_buffer) > 1000:
                    instance.log_buffer = instance.log_buffer[-1000:]
                
                # æ£€æµ‹å¯åŠ¨å®Œæˆ
                if "Application startup complete" in line or "Uvicorn running" in line:
                    instance.status = ModelStatus.RUNNING
                
                # æ£€æµ‹é”™è¯¯
                if "error" in line.lower() and instance.status == ModelStatus.STARTING:
                    instance.status = ModelStatus.ERROR
                    instance.error_message = line.strip()
        
        except Exception as e:
            print(f"è¯»å–æ—¥å¿—å‡ºé”™: {e}")
    
    async def stop_model(self, model_id: str) -> bool:
        """åœæ­¢æ¨¡å‹"""
        instance = self._instances.get(model_id)
        if not instance:
            raise ValueError(f"æ¨¡å‹å®ä¾‹ {model_id} ä¸å­˜åœ¨")
        
        if not instance.process or not instance.pid:
            # å·²ç»åœæ­¢
            instance.status = ModelStatus.STOPPED
            return True
        
        try:
            instance.status = ModelStatus.STOPPING
            
            # å°è¯•ä¼˜é›…å…³é—­
            process = psutil.Process(instance.pid)
            process.terminate()
            
            # ç­‰å¾…æœ€å¤š 10 ç§’
            try:
                process.wait(timeout=10)
            except psutil.TimeoutExpired:
                # å¼ºåˆ¶æ€æ­»
                process.kill()
                process.wait(timeout=5)
            
            instance.status = ModelStatus.STOPPED
            instance.pid = None
            
            # é‡Šæ”¾ç«¯å£
            self._used_ports.discard(instance.port)
            
            return True
        
        except Exception as e:
            instance.error_message = f"åœæ­¢å¤±è´¥: {str(e)}"
            return False
    
    async def remove_model(self, model_id: str) -> bool:
        """ç§»é™¤æ¨¡å‹å®ä¾‹"""
        if model_id in self._instances:
            # å…ˆåœæ­¢
            await self.stop_model(model_id)
            # ä»å­—å…¸ä¸­ç§»é™¤
            del self._instances[model_id]
            return True
        return False
    
    def get_model(self, model_id: str) -> Optional[ModelInfo]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        instance = self._instances.get(model_id)
        return instance.to_model_info() if instance else None
    
    def list_models(self) -> List[ModelInfo]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        return [inst.to_model_info() for inst in self._instances.values()]
    
    def get_logs(self, model_id: str, lines: int = 100) -> List[str]:
        """è·å–æ¨¡å‹æ—¥å¿—"""
        instance = self._instances.get(model_id)
        if not instance:
            return []
        return instance.log_buffer[-lines:]
    
    async def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰å®ä¾‹"""
        for model_id in list(self._instances.keys()):
            await self.stop_model(model_id)

