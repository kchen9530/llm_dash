# LLM Dashboard Configuration

# Server Settings
HOST=0.0.0.0
PORT=7860

# Mode: 'cpu' for local Mac (8GB RAM), 'gpu' for remote GPU server
DEPLOYMENT_MODE=cpu

# CPU Mode Settings (for Mac with limited RAM)
MAX_MODEL_SIZE_GB=2
DEFAULT_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# GPU Mode Settings (for future remote server)
GPU_SERVER_URL=
GPU_SERVER_API_KEY=

# HuggingFace Model Cache (downloads will be stored here)
HF_HOME=./models_cache
TRANSFORMERS_CACHE=./models_cache

# Network Settings (for China/Proxy users)
# Your SOCKS5 proxy should work automatically via TUN mode
# If you need to set proxy explicitly for HF downloads:
# HTTP_PROXY=socks5://127.0.0.1:1080
# HTTPS_PROXY=socks5://127.0.0.1:1080
# HF_ENDPOINT=https://huggingface.co  # or mirror if needed
